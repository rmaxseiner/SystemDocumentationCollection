#!/usr/bin/env python3
"""
Legacy processor containing the existing analyze_infrastructure.py logic.
This processor will be gradually replaced with new modular processors.
Adapted to work with the new processor pattern.
"""

import json
import yaml
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from collections import defaultdict, Counter

from .base_processor import BaseProcessor, ProcessingResult


class ExistingProcessor(BaseProcessor):
    """
    Legacy processor that contains all the existing analysis logic.
    This will be replaced with modular processors over time.
    """

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.data_dir = Path(config.get('data_dir', 'collected_data'))
        self.config_dir = Path(config.get('config_dir', 'collected_configs'))
        self.output_dir = config.get('output_dir', 'analysis_output')

    def validate_config(self) -> bool:
        """Validate processor configuration"""
        # Basic validation - directories will be created if they don't exist
        return True

    def process(self, collected_data: Dict[str, Any]) -> ProcessingResult:
        """
        Process collected data using the existing analyzer logic
        
        Args:
            collected_data: Dictionary containing results from collectors
            
        Returns:
            ProcessingResult: Contains analyzed data or error information
        """
        try:
            self.logger.info("Starting legacy analysis processing")
            
            # Create analyzer instance with the collected data
            analyzer = EnhancedInfrastructureAnalyzer(
                str(self.data_dir), 
                str(self.config_dir)
            )
            
            # If we have collected_data passed in, use it instead of loading from files
            if collected_data:
                analyzer.systems = self._extract_system_data(collected_data)
                analyzer.configurations = self._extract_config_data(collected_data)
            
            # Generate analysis outputs
            output_path, llm_context = analyzer.save_enhanced_outputs(self.output_dir)
            
            # Create result data
            result_data = {
                'output_directory': str(output_path),
                'llm_context_length': len(llm_context),
                'systems_analyzed': len(analyzer.systems),
                'configurations_analyzed': len(analyzer.configurations)
            }
            
            return ProcessingResult(
                success=True,
                data=result_data,
                metadata={
                    'processor_type': 'existing_analyzer',
                    'output_files': self._list_output_files(output_path)
                }
            )
            
        except Exception as e:
            self.logger.error(f"Processing failed: {e}")
            return ProcessingResult(
                success=False,
                error=str(e),
                metadata={'processor_type': 'existing_analyzer'}
            )

    def _extract_system_data(self, collected_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract system data from collected results"""
        systems = {}
        
        for system_name, collection_result in collected_data.items():
            if hasattr(collection_result, 'success') and collection_result.success:
                systems[system_name] = collection_result.data
            elif isinstance(collection_result, dict) and collection_result.get('success'):
                systems[system_name] = collection_result.get('data', {})
                
        return systems

    def _extract_config_data(self, collected_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract configuration data from collected results"""
        # For now, return empty dict - will be populated when we add config collectors
        return {}

    def _list_output_files(self, output_path: Path) -> List[str]:
        """List generated output files"""
        if not output_path.exists():
            return []
        
        files = []
        for file_path in output_path.iterdir():
            if file_path.is_file():
                files.append(file_path.name)
        
        return files


# Copy the existing analyzer class with minimal changes
class EnhancedInfrastructureAnalyzer:
    """Enhanced analyzer that processes both system state and configuration data"""

    def __init__(self, data_dir: str = "collected_data", config_dir: str = "collected_configs"):
        self.data_dir = Path(data_dir)
        self.config_dir = Path(config_dir)
        self.systems = {}
        self.configurations = {}
        self.load_all_data()

    def load_all_data(self):
        """Load both system state and configuration data"""
        print(f"üìÇ Loading data from {self.data_dir} and {self.config_dir}")

        # Load system state data (existing functionality)
        self.load_system_data()

        # Load configuration files (new functionality)
        self.load_configuration_data()

    def load_system_data(self):
        """Load system state data (Docker, Proxmox, etc.)"""
        if not self.data_dir.exists():
            print(f"   ‚ö†Ô∏è  System data directory not found: {self.data_dir}")
            return

        # Find latest file for each system
        system_files = {}

        for json_file in self.data_dir.glob("*.json"):
            try:
                system_name = json_file.stem.split('_')[0]
                if system_name not in system_files or json_file.stat().st_mtime > system_files[
                    system_name].stat().st_mtime:
                    system_files[system_name] = json_file
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Skipping {json_file}: {e}")

        # Load the latest file for each system
        for system_name, json_file in system_files.items():
            try:
                with open(json_file) as f:
                    data = json.load(f)

                if data.get('success', False):
                    self.systems[system_name] = data.get('data', {})
                    print(f"   ‚úÖ Loaded {system_name}: {json_file.name}")
                else:
                    print(f"   ‚ö†Ô∏è  {system_name}: Collection was not successful")

            except Exception as e:
                print(f"   ‚ùå Failed to load {json_file}: {e}")

    def load_configuration_data(self):
        """Load configuration files (Prometheus, Grafana, etc.)"""
        if not self.config_dir.exists():
            print(f"   ‚ö†Ô∏è  Config directory not found: {self.config_dir}")
            return

        # Find latest config file for each system
        config_files = {}

        for json_file in self.config_dir.glob("*.json"):
            try:
                system_name = json_file.stem.split('_')[0]
                if system_name not in config_files or json_file.stat().st_mtime > config_files[
                    system_name].stat().st_mtime:
                    config_files[system_name] = json_file
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Skipping config {json_file}: {e}")

        # Load configuration files
        for system_name, json_file in config_files.items():
            try:
                with open(json_file) as f:
                    data = json.load(f)

                if data.get('success', False):
                    self.configurations[system_name] = data.get('data', {})
                    print(f"   ‚úÖ Loaded config {system_name}: {json_file.name}")
                else:
                    print(f"   ‚ö†Ô∏è  {system_name}: Config collection was not successful")

            except Exception as e:
                print(f"   ‚ùå Failed to load config {json_file}: {e}")

    # NOTE: The rest of the methods from EnhancedInfrastructureAnalyzer would go here
    # For brevity, I'm including just the key methods needed for save_enhanced_outputs
    # The full implementation would include all methods from the original class
    
    def save_enhanced_outputs(self, output_dir: str = "analysis_output"):
        """Save enhanced analysis outputs including RAG export"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # For now, create minimal output to test the structure
        summary = {'timestamp': datetime.now().isoformat(), 'systems': len(self.systems)}
        llm_context = f"# Infrastructure Analysis\\n\\nAnalyzed {len(self.systems)} systems at {summary['timestamp']}"

        # Save basic summary
        with open(output_path / "infrastructure_summary.json", 'w') as f:
            json.dump(summary, f, indent=2, default=str)

        # Save basic context
        with open(output_path / "llm_context.md", 'w') as f:
            f.write(llm_context)

        print(f"üíæ Analysis outputs saved to {output_path.absolute()}")
        
        return output_path, llm_context